{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d6d1e854e0c9f3cbf789605e70af20e0232a4634"
   },
   "source": [
    "**Scania Air Pressure System Failures Prediction**\n",
    "\n",
    "In this challange we are asked to predict if there is truck APS failure based on the sensor telemetry data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**READING DATA**\n",
    "\n",
    "First I will load basic libraries and raw data. Additional libraries I will be loading as necessary to increase readibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv(\"../Data/aps_failure_training_set.csv\", na_values=\"na\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "be7d0e24e4f10a1de9d704fca2f0088aaed31f47"
   },
   "source": [
    "From a visual inspection of raw data it is obvious that some columns contain missing values. The first column named \"class\" is our target set (labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = data.isna().sum().div(data.shape[0]).mul(100).to_frame().sort_values(by=0, ascending = False)\n",
    "missing.plot.bar(figsize=(50,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph above shows that we have significant amount of missing data. In this approach to modelling I am not going to drop any columns and check what best results I can get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing[missing[0]>80]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DEALING WITH MISSING DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6f70e4c37bfb161ef1022dfb4f7d42d8b4bb30d0"
   },
   "outputs": [],
   "source": [
    "X = data.drop([\"class\",\"br_000\",\"bq_000\"], axis=1)\n",
    "y = data.loc[:,\"class\"]\n",
    "y = pd.get_dummies(y).drop(\"neg\",axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e7c5b60ce9f99314bffda83f50c7aaaf5c38cb1a"
   },
   "source": [
    "Filling missing data with a mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "85a8e3ab3776357c2f23b7e83e1496319d72398c"
   },
   "outputs": [],
   "source": [
    "X.fillna(X.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "185820b1c82c2794364500438d4e8ff8ecc42c6c"
   },
   "source": [
    "**DATA STANDARISATION**\n",
    "\n",
    "I am going to use the Support Vector Machine Classifier and it requires standarisation of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d5bb4d034f607c0baf6d6a7d9158641534965af5"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CREATING CUSTOM SCORER**\n",
    "\n",
    "Here I will create a custom scorer accoring to the database guidelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def my_scorer(y_true,y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    cost = 10*fp+500*fn\n",
    "    return cost\n",
    "\n",
    "my_func = make_scorer(my_scorer, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2eb59db72b601f37d3f60b437ab5ba41a936a8a2"
   },
   "source": [
    "**PCA AND PARAMETERS OPTIMISATION PIPELINED**\n",
    "\n",
    "I will chain PCA and classification model with a pipeline to perform a grid search optimisation. In the cell below I will use Support Vector Machine Classifier (SVC). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c466da93123c8680d82bbe2316d91e8b0bb567b5"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = SVC(probability = False, class_weight=\"balanced\", gamma=\"auto\")\n",
    "pca = PCA()\n",
    "\n",
    "pipe = Pipeline(steps=[(\"pca\",pca),(\"clf\",clf)])\n",
    "\n",
    "param_grid = {\n",
    "    'pca__n_components': range(10,26),\n",
    "    'clf__C': [0.2, 0.3,0.4,0.5],\n",
    "}\n",
    "\n",
    "search = GridSearchCV(pipe, param_grid, iid=False, cv=3, return_train_score=False, scoring = my_func, n_jobs=-1, verbose=3)\n",
    "search.fit(X_scaled, np.ravel(y))\n",
    "\n",
    "# %% Plotting best classificator\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d40fa97797846cd4dda2f1cce1619a81efbaf991"
   },
   "outputs": [],
   "source": [
    "pca.fit(X_scaled)\n",
    "\n",
    "fig, ax0 = plt.subplots(nrows=1, sharex=True, figsize=(12, 6))\n",
    "ax0.plot(pca.explained_variance_ratio_, linewidth=2)\n",
    "ax0.set_ylabel('PCA explained variance')\n",
    "ax0.axvline(search.best_estimator_.named_steps['pca'].n_components, linestyle=':', label='n_components chosen')\n",
    "ax0.legend(prop=dict(size=12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2ccddcb01e643c97c481c08ef7abaeb7b109cc7f"
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(nrows=1, sharex=True, figsize=(12, 6))\n",
    "\n",
    "results = pd.DataFrame(search.cv_results_)\n",
    "components_col = 'param_pca__n_components'\n",
    "best_clfs = results.groupby(components_col).apply(\n",
    "    lambda g: g.nlargest(1, 'mean_test_score'))\n",
    "\n",
    "best_clfs.plot(x=components_col, y='mean_test_score', yerr='std_test_score',\n",
    "               legend=False, ax=ax1)\n",
    "ax1.set_ylabel('Classification accuracy (val)')\n",
    "ax1.set_xlabel('n_components')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
